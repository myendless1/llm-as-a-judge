from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM
model_path = "/vepfs-sha/liying/LLM_JUDGER/LLaMA-Factory/saves/chatglm3-6b_judge2/lora/sft/checkpoint-4500"

def load_model(model_path, device):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoPeftModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(device)
    return tokenizer, model

tokenizer, model = load_model(model_path, "cuda:1")

_system_msg = """
You are a helpful assistant in evaluating the quality of the responses of two assistants for a given instruction. Your goal is to select the better response for the given instruction. The two responses are generated by two different AI assistants respectively.
"""

_user_msg = """
After giving a brief explanation, select the A or B that is better for the given instruction. Output your decision and explanation in the Decision & Explanation section.\n\nHere are some rules of the evaluation:\n(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.\n(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.\n(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Assistant A's response and Assistant B's response are **equally likely** to be the better.\n\nYou should first provide a brief explanation of your evaluation, and then always end your response with \"Final Verdict: [[A]]\" or \"Final Verdict: [[B]]\" or \"Final Verdict: [[C]] for tie\" \n[[A]] means the Assistant A's response is better, [[B]] means the Assistant B's response is better, [[Tie]] means the responses are both ok or none of them is suitable.\nDo NOT output any other words.\nDo NOT say your verdict at the beginning. You better do reasoning and thinking **before** claiming which is better.\n\n# Instruction:\nSummarize the following post\n\nTitle: Is this a major red flag?\n\nPost: I have been dating Amanda (lets call her that) for over a month. We had gotten pretty far (hooked up, etc), things were looking up.\n\nWe had plans to go to a local bar with a bunch of mutual friends. She texts me a couple of hours saying that she's sorry that she can't stay for very long, she's hanging with friends who are in town for only a couple days. I say Ok. \n\nShe gets there, greets me and says \"Listen, my good friend Andy is very protective of me and gets defensive and honestly I don't want to deal with his ranting about how I'm dating someone when he's drunk later on tonight. So don't mention to him that were dating, no PDA etc. Ok?\"\n\nI agree and really think nothing of it. So far there have been no trust issues in the relationship. This where it gets fucking weird\n\nBut she almost immediately ignores myself our mutual friends and talks to Andy most of the night. So eventually I go over and talk to her and Andy. We're shooting the shit etc. The conversation goes on and I'm forced to introduce myself. She walks off at some point, I say nice meeting you and turn back to my friends. \n\n I can't really remember any of her other friends being there but the beer was getting served to me pretty quick. I definitely did not meet any of them. They leave and I can't remember if they left alone or with other people.\n\n# Assistant A's response:\n Girl went to bar, didn't hang out with me, did most of the talking, acted like she was hanging out with friends, ignores me and nearly ignores a mutual friend.\n\n# Assistant B's response:\n Girl wants me to not mention that we are dating. She makes plans with friends (who I did not meet and am not friends with) and only talks to me while I'm talking to friends.\n\n# Decision & Explanation:
"""
system_msg = "<|im_start|>system\n{content}<|im_end|>\n".format(content=_system_msg)
user_msg = "<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n".format(content=_user_msg)

def generate_response(user_msg, system_msg):
    input_text = system_msg + user_msg
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=2048).to("cuda:1")
    output_ids = model.generate(**inputs, max_new_tokens=2048)
    output = tokenizer.decode(output_ids[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True, spaces_between_special_tokens=False)
    return output

response = generate_response(user_msg, system_msg)
print(response)